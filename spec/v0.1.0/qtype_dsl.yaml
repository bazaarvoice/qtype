
version: "0.1.1"

prompt:
  name: generate_summary
  input_vars:
    - name: transcript
      type: text
  output_vars:
    - name: summary
      type: text
  template: |
    Summarize the following meeting transcript: {{ transcript }}
  registry: github://myorg/prompts@generate_summary_v2

guardrail:
  name: quality_check
  type: llm_judge
  prompt: |
    Evaluate the assistantâ€™s response. Was it helpful, complete, and based on provided context?
  inputs:
    - name: response
      type: text
    - name: context
      type: text
  outputs:
    - name: evaluation
      type: text

subflow:
  name: summarize_meeting
  description: Generate a summary and validate quality.
  parameters:
    - name: transcript
      type: text
  steps:
    - id: summary
      type: prompt
      uses: generate_summary
      inputs:
        transcript: "{{ transcript }}"
    - id: quality
      type: guardrail
      uses: quality_check
      inputs:
        response: "{{ steps.summary.outputs.summary }}"
        context: "{{ transcript }}"
  outputs:
    summary: "{{ steps.summary.outputs.summary }}"
    evaluation: "{{ steps.quality.outputs.evaluation }}"

input:
  name: meeting_transcript
  type: file
  display_type: textarea
  display_hints:
    placeholder: "Paste the meeting transcript here..."
    max_length: 5000
    help_text: "This will be used to generate a concise summary."

flow:
  name: meeting_summary_flow
  mode: chat
  inputs:
    - meeting_transcript
  steps:
    - id: run_summary
      type: subflow
      uses: summarize_meeting
      inputs:
        transcript: "{{ inputs.meeting_transcript }}"
      telemetry:
        trace: true
        custom_tags:
          user_role: "beta_tester"
          region: "us-east"
